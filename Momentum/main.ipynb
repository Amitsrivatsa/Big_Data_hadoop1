{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.DataProcessor import *\n",
    "from Modules.ForwardLayers import *\n",
    "from Modules.BackwardLayers import *\n",
    "from Modules.Loss import *\n",
    "from Modules.Trainner import *\n",
    "from Modules.optim import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (49000, 3, 32, 32)\n",
      "y_train:  (49000,)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Fetching the datasets\n",
    "data = get_CIFAR10_data()\n",
    "\n",
    "# Printing the dimensions and shape of the datasets\n",
    "for k, v in data.items():\n",
    "    print ('%s: ' % k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the Convolutional Neural Network\n",
    "\n",
    "class ConvNet(object):\n",
    "    \"\"\" Architecture: conv - relu - 2x2 max pool - affine - relu - affine - softmax\"\"\"\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_filters=32, filter_size=7,\n",
    "                 hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0,\n",
    "                 dtype=np.float32, use_batchnorm=False):\n",
    "        \n",
    "        # Initialization\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "        self.params = {}  # Dictionary to pack weights and bias\n",
    "        self.reg = reg\n",
    "        self.dtype = dtype\n",
    "        # Size of the input\n",
    "        C, H, W = input_dim\n",
    "        # Size of filters\n",
    "        F = num_filters\n",
    "        filter_height = filter_size\n",
    "        filter_width = filter_size\n",
    "        # Stride\n",
    "        stride_conv = 1\n",
    "        # Pad                                          \n",
    "        P = (filter_size - 1) / 2\n",
    "        \n",
    "        # Output Activations size\n",
    "        Hc =((H + 2 * P - filter_height) / stride_conv )+ 1\n",
    "        Wc = ((W + 2 * P - filter_width) / stride_conv) + 1\n",
    "        \n",
    "        # Initializing the weights with a given standard deviations\n",
    "        W1 = weight_scale * np.random.randn(F, C, filter_height, filter_width)\n",
    "        # Initialzing the bias\n",
    "        b1 = np.zeros(F)\n",
    "        \n",
    "        # Parameters for pooling\n",
    "        width_pool = 2\n",
    "        height_pool = 2\n",
    "        stride_pool = 2\n",
    "        Hp = ((Hc - height_pool) / stride_pool) + 1\n",
    "        Wp = ((Wc - width_pool) / stride_pool )+ 1\n",
    "        \n",
    "        Hp=int(Hp)  # Type casting into int\n",
    "        Wp=int(Wp)\n",
    "        \n",
    "        # Hidden Affine layer Parameters\n",
    "        Hh = hidden_dim\n",
    "        W2 = weight_scale * np.random.randn(F * Hp * Wp, Hh)\n",
    "        b2 = np.zeros(Hh)\n",
    "        \n",
    "        # Output affine layer Parameters\n",
    "        Hc = num_classes\n",
    "        W3 = weight_scale * np.random.randn(Hh, Hc)\n",
    "        b3 = np.zeros(Hc)\n",
    "\n",
    "        self.params.update({'W1': W1,\n",
    "                            'W2': W2,      # Packing all weights into a dictionary \n",
    "                            'W3': W3,\n",
    "                            'b1': b1,\n",
    "                            'b2': b2,\n",
    "                            'b3': b3})\n",
    "        \n",
    "        for k, v in self.params.items():     # Type conversion\n",
    "            self.params[k] = v.astype(dtype)   \n",
    "    \n",
    "    def loss(self, X, y=None):\n",
    "        \n",
    "        \"\"\"Evaluate loss and gradient for the three-layer convolutional network.\"\"\"\n",
    "\n",
    "        \n",
    "        X = X.astype(self.dtype) # Type casting into suitable type for numpy\n",
    "        \n",
    "        mode = 'test' if y is None else 'train' # Setting mode- Test/Train\n",
    "\n",
    "        N = X.shape[0]                        \n",
    "\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W3, b3 = self.params['W3'], self.params['b3']\n",
    "\n",
    "        # pass conv_param to the forward pass for the convolutional layer\n",
    "        filter_size = W1.shape[2]\n",
    "        conv_param = {'stride': 1, 'pad':(filter_size-1)/2} \n",
    "\n",
    "        # pass pool_param to the forward pass for the max-pooling layer\n",
    "        pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "        scores = None\n",
    "       \n",
    "        # Forward into the conv layer\n",
    "        x = X\n",
    "        w = W1\n",
    "        b = b1\n",
    "        \n",
    "\n",
    "        \n",
    "        conv_layer, cache_conv_layer = conv_relu_pool_forward(x, w, b, conv_param, pool_param)\n",
    "\n",
    "        N, F, Hp, Wp = conv_layer.shape  # output shape\n",
    "        \n",
    "\n",
    "\n",
    "        # Forward into the hidden layer\n",
    "        \n",
    "        x = conv_layer.reshape((N, F * Hp * Wp))\n",
    "        w = W2\n",
    "        b = b2 \n",
    "        \n",
    "\n",
    "        hidden_layer, cache_hidden_layer = affine_relu_forward(x, w, b)\n",
    "        N, Hh = hidden_layer.shape\n",
    "        \n",
    "\n",
    "\n",
    "        # Forward into the linear output layer\n",
    "       \n",
    "        x = hidden_layer\n",
    "        w = W3\n",
    "        b = b3\n",
    "        \n",
    "\n",
    "        scores, cache_scores = affine_forward(x, w, b)\n",
    "\n",
    "\n",
    "        if y is None:               # If it is a test, return the scores\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0, {}\n",
    "       \n",
    "        data_loss, dscores = softmax_loss(scores, y)\n",
    "        reg_loss = 0.5 * self.reg * np.sum(W1**2)\n",
    "        reg_loss += 0.5 * self.reg * np.sum(W2**2)\n",
    "        reg_loss += 0.5 * self.reg * np.sum(W3**2)\n",
    "        loss = data_loss + reg_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        grads = {}\n",
    "        # Backprop into output layer\n",
    "        dx3, dW3, db3 = affine_backward(dscores, cache_scores)\n",
    "        dW3 += self.reg * W3\n",
    "\n",
    "        # Backprop into first layer\n",
    "        dx2, dW2, db2 = affine_relu_backward(dx3, cache_hidden_layer)\n",
    "\n",
    "        dW2 += self.reg * W2\n",
    "\n",
    "        # Backprop into the conv layer\n",
    "        dx2 = dx2.reshape(N, F, Hp, Wp)\n",
    "        dx, dW1, db1 = conv_relu_pool_backward(dx2, cache_conv_layer)\n",
    "        dW1 += self.reg * W1\n",
    "\n",
    "        grads.update({'W1': dW1,\n",
    "                      'b1': db1,\n",
    "                      'W2': dW2,\n",
    "                      'b2': db2,\n",
    "                      'W3': dW3,\n",
    "                      'b3': db3})\n",
    "\n",
    "      \n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 300) loss: 2.547069\n",
      "(Epoch 0 / 15) train acc: 0.084000; val_acc: 0.100000\n",
      "(Iteration 2 / 300) loss: 2.472701\n",
      "(Iteration 3 / 300) loss: 2.268246\n",
      "(Iteration 4 / 300) loss: 2.372716\n",
      "(Iteration 5 / 300) loss: 2.311258\n",
      "(Iteration 6 / 300) loss: 2.206895\n",
      "(Iteration 7 / 300) loss: 2.345970\n",
      "(Iteration 8 / 300) loss: 2.061764\n",
      "(Iteration 9 / 300) loss: 2.249708\n",
      "(Iteration 10 / 300) loss: 2.039946\n",
      "(Iteration 11 / 300) loss: 2.092065\n",
      "(Iteration 12 / 300) loss: 2.062077\n",
      "(Iteration 13 / 300) loss: 2.101921\n",
      "(Iteration 14 / 300) loss: 2.124978\n",
      "(Iteration 15 / 300) loss: 2.082551\n",
      "(Iteration 16 / 300) loss: 1.949124\n",
      "(Iteration 17 / 300) loss: 2.085685\n",
      "(Iteration 18 / 300) loss: 1.906191\n",
      "(Iteration 19 / 300) loss: 1.841485\n",
      "(Iteration 20 / 300) loss: 1.654693\n",
      "(Epoch 1 / 15) train acc: 0.332000; val_acc: 0.273000\n",
      "(Iteration 21 / 300) loss: 1.891277\n",
      "(Iteration 22 / 300) loss: 1.994086\n",
      "(Iteration 23 / 300) loss: 1.974085\n",
      "(Iteration 24 / 300) loss: 2.012010\n",
      "(Iteration 25 / 300) loss: 1.715841\n",
      "(Iteration 26 / 300) loss: 1.898850\n",
      "(Iteration 27 / 300) loss: 1.855340\n",
      "(Iteration 28 / 300) loss: 1.680565\n",
      "(Iteration 29 / 300) loss: 1.810906\n",
      "(Iteration 30 / 300) loss: 1.651897\n",
      "(Iteration 31 / 300) loss: 1.661908\n",
      "(Iteration 32 / 300) loss: 1.682085\n",
      "(Iteration 33 / 300) loss: 1.609012\n",
      "(Iteration 34 / 300) loss: 1.798718\n",
      "(Iteration 35 / 300) loss: 1.919123\n",
      "(Iteration 36 / 300) loss: 1.711953\n",
      "(Iteration 37 / 300) loss: 1.530222\n",
      "(Iteration 38 / 300) loss: 1.649696\n",
      "(Iteration 39 / 300) loss: 1.535596\n",
      "(Iteration 40 / 300) loss: 1.764307\n",
      "(Epoch 2 / 15) train acc: 0.441000; val_acc: 0.335000\n",
      "(Iteration 41 / 300) loss: 1.345766\n",
      "(Iteration 42 / 300) loss: 1.942994\n",
      "(Iteration 43 / 300) loss: 1.799915\n",
      "(Iteration 44 / 300) loss: 1.760012\n",
      "(Iteration 45 / 300) loss: 1.320155\n",
      "(Iteration 46 / 300) loss: 1.418725\n",
      "(Iteration 47 / 300) loss: 1.489479\n",
      "(Iteration 48 / 300) loss: 1.656991\n",
      "(Iteration 49 / 300) loss: 1.580326\n",
      "(Iteration 50 / 300) loss: 1.324751\n",
      "(Iteration 51 / 300) loss: 1.789377\n",
      "(Iteration 52 / 300) loss: 1.418154\n",
      "(Iteration 53 / 300) loss: 1.353321\n",
      "(Iteration 54 / 300) loss: 1.560912\n",
      "(Iteration 55 / 300) loss: 1.777850\n",
      "(Iteration 56 / 300) loss: 1.307072\n",
      "(Iteration 57 / 300) loss: 1.556663\n",
      "(Iteration 58 / 300) loss: 1.257680\n",
      "(Iteration 59 / 300) loss: 1.444456\n",
      "(Iteration 60 / 300) loss: 1.494169\n",
      "(Epoch 3 / 15) train acc: 0.484000; val_acc: 0.316000\n",
      "(Iteration 61 / 300) loss: 1.323604\n",
      "(Iteration 62 / 300) loss: 1.362402\n",
      "(Iteration 63 / 300) loss: 1.156475\n",
      "(Iteration 64 / 300) loss: 1.420513\n",
      "(Iteration 65 / 300) loss: 1.781066\n",
      "(Iteration 66 / 300) loss: 1.523921\n",
      "(Iteration 67 / 300) loss: 1.371354\n",
      "(Iteration 68 / 300) loss: 1.461187\n",
      "(Iteration 69 / 300) loss: 1.230703\n",
      "(Iteration 70 / 300) loss: 1.243231\n",
      "(Iteration 71 / 300) loss: 1.684646\n",
      "(Iteration 72 / 300) loss: 1.272162\n",
      "(Iteration 73 / 300) loss: 1.133371\n",
      "(Iteration 74 / 300) loss: 1.151163\n",
      "(Iteration 75 / 300) loss: 1.294234\n",
      "(Iteration 76 / 300) loss: 1.132022\n",
      "(Iteration 77 / 300) loss: 1.110214\n",
      "(Iteration 78 / 300) loss: 1.319430\n",
      "(Iteration 79 / 300) loss: 1.408999\n",
      "(Iteration 80 / 300) loss: 1.172492\n",
      "(Epoch 4 / 15) train acc: 0.553000; val_acc: 0.320000\n",
      "(Iteration 81 / 300) loss: 1.656194\n",
      "(Iteration 82 / 300) loss: 1.060117\n",
      "(Iteration 83 / 300) loss: 0.927915\n",
      "(Iteration 84 / 300) loss: 0.928398\n",
      "(Iteration 85 / 300) loss: 1.191291\n",
      "(Iteration 86 / 300) loss: 1.314155\n",
      "(Iteration 87 / 300) loss: 1.283999\n",
      "(Iteration 88 / 300) loss: 1.600758\n",
      "(Iteration 89 / 300) loss: 1.212130\n",
      "(Iteration 90 / 300) loss: 1.036844\n",
      "(Iteration 91 / 300) loss: 1.001536\n",
      "(Iteration 92 / 300) loss: 1.222412\n",
      "(Iteration 93 / 300) loss: 1.435796\n",
      "(Iteration 94 / 300) loss: 1.154575\n",
      "(Iteration 95 / 300) loss: 1.033420\n",
      "(Iteration 96 / 300) loss: 1.325704\n",
      "(Iteration 97 / 300) loss: 1.596882\n",
      "(Iteration 98 / 300) loss: 1.206267\n",
      "(Iteration 99 / 300) loss: 0.908284\n",
      "(Iteration 100 / 300) loss: 1.033697\n",
      "(Epoch 5 / 15) train acc: 0.631000; val_acc: 0.344000\n",
      "(Iteration 101 / 300) loss: 1.210062\n",
      "(Iteration 102 / 300) loss: 0.884091\n",
      "(Iteration 103 / 300) loss: 0.696847\n",
      "(Iteration 104 / 300) loss: 1.094722\n",
      "(Iteration 105 / 300) loss: 1.201126\n",
      "(Iteration 106 / 300) loss: 0.957287\n",
      "(Iteration 107 / 300) loss: 1.306170\n",
      "(Iteration 108 / 300) loss: 1.367794\n",
      "(Iteration 109 / 300) loss: 0.761544\n",
      "(Iteration 110 / 300) loss: 0.937684\n",
      "(Iteration 111 / 300) loss: 1.275295\n",
      "(Iteration 112 / 300) loss: 1.290671\n",
      "(Iteration 113 / 300) loss: 0.811417\n",
      "(Iteration 114 / 300) loss: 1.008592\n",
      "(Iteration 115 / 300) loss: 1.143337\n",
      "(Iteration 116 / 300) loss: 0.963141\n",
      "(Iteration 117 / 300) loss: 1.068483\n",
      "(Iteration 118 / 300) loss: 0.795935\n",
      "(Iteration 119 / 300) loss: 1.222020\n",
      "(Iteration 120 / 300) loss: 0.994644\n",
      "(Epoch 6 / 15) train acc: 0.688000; val_acc: 0.342000\n",
      "(Iteration 121 / 300) loss: 0.776070\n",
      "(Iteration 122 / 300) loss: 0.839298\n",
      "(Iteration 123 / 300) loss: 0.975987\n",
      "(Iteration 124 / 300) loss: 0.900720\n",
      "(Iteration 125 / 300) loss: 0.680949\n",
      "(Iteration 126 / 300) loss: 0.910502\n",
      "(Iteration 127 / 300) loss: 1.051303\n",
      "(Iteration 128 / 300) loss: 0.946041\n",
      "(Iteration 129 / 300) loss: 1.131549\n",
      "(Iteration 130 / 300) loss: 0.896856\n",
      "(Iteration 131 / 300) loss: 0.667666\n",
      "(Iteration 132 / 300) loss: 1.048953\n",
      "(Iteration 133 / 300) loss: 0.630246\n",
      "(Iteration 134 / 300) loss: 0.665101\n",
      "(Iteration 135 / 300) loss: 0.938719\n",
      "(Iteration 136 / 300) loss: 0.585172\n",
      "(Iteration 137 / 300) loss: 0.632123\n",
      "(Iteration 138 / 300) loss: 0.826893\n",
      "(Iteration 139 / 300) loss: 0.573527\n",
      "(Iteration 140 / 300) loss: 0.466944\n",
      "(Epoch 7 / 15) train acc: 0.766000; val_acc: 0.372000\n",
      "(Iteration 141 / 300) loss: 0.570542\n",
      "(Iteration 142 / 300) loss: 0.695594\n",
      "(Iteration 143 / 300) loss: 0.527212\n",
      "(Iteration 144 / 300) loss: 0.709740\n",
      "(Iteration 145 / 300) loss: 0.608463\n",
      "(Iteration 146 / 300) loss: 0.486282\n",
      "(Iteration 147 / 300) loss: 0.536695\n",
      "(Iteration 148 / 300) loss: 0.996572\n",
      "(Iteration 149 / 300) loss: 0.656142\n",
      "(Iteration 150 / 300) loss: 0.290941\n",
      "(Iteration 151 / 300) loss: 0.580489\n",
      "(Iteration 152 / 300) loss: 0.853619\n",
      "(Iteration 153 / 300) loss: 0.561772\n",
      "(Iteration 154 / 300) loss: 0.498354\n",
      "(Iteration 155 / 300) loss: 0.594292\n",
      "(Iteration 156 / 300) loss: 0.501034\n",
      "(Iteration 157 / 300) loss: 0.582624\n",
      "(Iteration 158 / 300) loss: 0.388008\n",
      "(Iteration 159 / 300) loss: 0.669697\n",
      "(Iteration 160 / 300) loss: 0.679140\n",
      "(Epoch 8 / 15) train acc: 0.804000; val_acc: 0.337000\n",
      "(Iteration 161 / 300) loss: 0.699039\n",
      "(Iteration 162 / 300) loss: 0.667090\n",
      "(Iteration 163 / 300) loss: 0.598642\n",
      "(Iteration 164 / 300) loss: 0.665591\n",
      "(Iteration 165 / 300) loss: 0.657732\n",
      "(Iteration 166 / 300) loss: 0.438924\n",
      "(Iteration 167 / 300) loss: 0.421713\n",
      "(Iteration 168 / 300) loss: 0.534161\n",
      "(Iteration 169 / 300) loss: 0.370063\n",
      "(Iteration 170 / 300) loss: 0.510352\n",
      "(Iteration 171 / 300) loss: 0.528192\n",
      "(Iteration 172 / 300) loss: 0.421128\n",
      "(Iteration 173 / 300) loss: 0.645783\n",
      "(Iteration 174 / 300) loss: 0.426683\n",
      "(Iteration 175 / 300) loss: 0.522417\n",
      "(Iteration 176 / 300) loss: 0.351062\n",
      "(Iteration 177 / 300) loss: 0.468855\n",
      "(Iteration 178 / 300) loss: 0.316179\n",
      "(Iteration 179 / 300) loss: 0.471225\n",
      "(Iteration 180 / 300) loss: 0.393553\n",
      "(Epoch 9 / 15) train acc: 0.891000; val_acc: 0.384000\n",
      "(Iteration 181 / 300) loss: 0.268100\n",
      "(Iteration 182 / 300) loss: 0.270451\n",
      "(Iteration 183 / 300) loss: 0.480724\n",
      "(Iteration 184 / 300) loss: 0.500288\n",
      "(Iteration 185 / 300) loss: 0.535658\n",
      "(Iteration 186 / 300) loss: 0.188614\n",
      "(Iteration 187 / 300) loss: 0.205504\n",
      "(Iteration 188 / 300) loss: 0.450773\n",
      "(Iteration 189 / 300) loss: 0.485339\n",
      "(Iteration 190 / 300) loss: 0.290001\n",
      "(Iteration 191 / 300) loss: 0.290372\n",
      "(Iteration 192 / 300) loss: 0.256487\n",
      "(Iteration 193 / 300) loss: 0.466779\n",
      "(Iteration 194 / 300) loss: 0.264605\n",
      "(Iteration 195 / 300) loss: 0.350879\n",
      "(Iteration 196 / 300) loss: 0.302946\n",
      "(Iteration 197 / 300) loss: 0.288418\n",
      "(Iteration 198 / 300) loss: 0.198555\n",
      "(Iteration 199 / 300) loss: 0.335242\n",
      "(Iteration 200 / 300) loss: 0.328739\n",
      "(Epoch 10 / 15) train acc: 0.882000; val_acc: 0.384000\n",
      "(Iteration 201 / 300) loss: 0.429682\n",
      "(Iteration 202 / 300) loss: 0.193029\n",
      "(Iteration 203 / 300) loss: 0.282522\n",
      "(Iteration 204 / 300) loss: 0.341028\n",
      "(Iteration 205 / 300) loss: 0.391260\n",
      "(Iteration 206 / 300) loss: 0.408871\n",
      "(Iteration 207 / 300) loss: 0.237769\n",
      "(Iteration 208 / 300) loss: 0.175357\n",
      "(Iteration 209 / 300) loss: 0.242560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 210 / 300) loss: 0.287349\n",
      "(Iteration 211 / 300) loss: 0.419837\n",
      "(Iteration 212 / 300) loss: 0.318259\n",
      "(Iteration 213 / 300) loss: 0.131243\n",
      "(Iteration 214 / 300) loss: 0.205255\n",
      "(Iteration 215 / 300) loss: 0.193463\n",
      "(Iteration 216 / 300) loss: 0.214964\n",
      "(Iteration 217 / 300) loss: 0.185772\n",
      "(Iteration 218 / 300) loss: 0.380382\n",
      "(Iteration 219 / 300) loss: 0.200161\n",
      "(Iteration 220 / 300) loss: 0.292223\n",
      "(Epoch 11 / 15) train acc: 0.946000; val_acc: 0.393000\n",
      "(Iteration 221 / 300) loss: 0.208697\n",
      "(Iteration 222 / 300) loss: 0.179249\n",
      "(Iteration 223 / 300) loss: 0.121562\n",
      "(Iteration 224 / 300) loss: 0.181951\n",
      "(Iteration 225 / 300) loss: 0.117823\n",
      "(Iteration 226 / 300) loss: 0.197756\n",
      "(Iteration 227 / 300) loss: 0.168282\n",
      "(Iteration 228 / 300) loss: 0.050468\n",
      "(Iteration 229 / 300) loss: 0.124183\n",
      "(Iteration 230 / 300) loss: 0.147932\n",
      "(Iteration 231 / 300) loss: 0.276480\n",
      "(Iteration 232 / 300) loss: 0.175535\n",
      "(Iteration 233 / 300) loss: 0.178385\n",
      "(Iteration 234 / 300) loss: 0.108831\n",
      "(Iteration 235 / 300) loss: 0.093023\n",
      "(Iteration 236 / 300) loss: 0.108641\n",
      "(Iteration 237 / 300) loss: 0.103919\n",
      "(Iteration 238 / 300) loss: 0.211476\n",
      "(Iteration 239 / 300) loss: 0.074390\n",
      "(Iteration 240 / 300) loss: 0.085645\n",
      "(Epoch 12 / 15) train acc: 0.962000; val_acc: 0.362000\n",
      "(Iteration 241 / 300) loss: 0.151189\n",
      "(Iteration 242 / 300) loss: 0.078705\n",
      "(Iteration 243 / 300) loss: 0.085123\n",
      "(Iteration 244 / 300) loss: 0.155587\n",
      "(Iteration 245 / 300) loss: 0.183605\n",
      "(Iteration 246 / 300) loss: 0.131314\n",
      "(Iteration 247 / 300) loss: 0.156797\n",
      "(Iteration 248 / 300) loss: 0.048328\n",
      "(Iteration 249 / 300) loss: 0.140163\n",
      "(Iteration 250 / 300) loss: 0.183044\n",
      "(Iteration 251 / 300) loss: 0.075150\n",
      "(Iteration 252 / 300) loss: 0.078939\n",
      "(Iteration 253 / 300) loss: 0.081926\n",
      "(Iteration 254 / 300) loss: 0.070419\n",
      "(Iteration 255 / 300) loss: 0.237176\n",
      "(Iteration 256 / 300) loss: 0.131140\n",
      "(Iteration 257 / 300) loss: 0.128412\n",
      "(Iteration 258 / 300) loss: 0.067347\n",
      "(Iteration 259 / 300) loss: 0.180278\n",
      "(Iteration 260 / 300) loss: 0.097218\n",
      "(Epoch 13 / 15) train acc: 0.953000; val_acc: 0.340000\n",
      "(Iteration 261 / 300) loss: 0.043494\n",
      "(Iteration 262 / 300) loss: 0.122161\n",
      "(Iteration 263 / 300) loss: 0.229376\n",
      "(Iteration 264 / 300) loss: 0.076447\n",
      "(Iteration 265 / 300) loss: 0.073681\n",
      "(Iteration 266 / 300) loss: 0.078908\n",
      "(Iteration 267 / 300) loss: 0.087659\n",
      "(Iteration 268 / 300) loss: 0.041552\n",
      "(Iteration 269 / 300) loss: 0.026948\n",
      "(Iteration 270 / 300) loss: 0.202335\n",
      "(Iteration 271 / 300) loss: 0.138072\n",
      "(Iteration 272 / 300) loss: 0.119770\n",
      "(Iteration 273 / 300) loss: 0.032221\n",
      "(Iteration 274 / 300) loss: 0.064896\n",
      "(Iteration 275 / 300) loss: 0.056086\n",
      "(Iteration 276 / 300) loss: 0.177477\n",
      "(Iteration 277 / 300) loss: 0.082393\n",
      "(Iteration 278 / 300) loss: 0.149878\n",
      "(Iteration 279 / 300) loss: 0.043377\n",
      "(Iteration 280 / 300) loss: 0.037215\n",
      "(Epoch 14 / 15) train acc: 0.970000; val_acc: 0.383000\n",
      "(Iteration 281 / 300) loss: 0.085172\n",
      "(Iteration 282 / 300) loss: 0.098962\n",
      "(Iteration 283 / 300) loss: 0.071004\n",
      "(Iteration 284 / 300) loss: 0.105041\n",
      "(Iteration 285 / 300) loss: 0.129684\n",
      "(Iteration 286 / 300) loss: 0.045000\n",
      "(Iteration 287 / 300) loss: 0.043813\n",
      "(Iteration 288 / 300) loss: 0.038097\n",
      "(Iteration 289 / 300) loss: 0.041262\n",
      "(Iteration 290 / 300) loss: 0.038968\n",
      "(Iteration 291 / 300) loss: 0.041557\n",
      "(Iteration 292 / 300) loss: 0.069420\n",
      "(Iteration 293 / 300) loss: 0.031606\n",
      "(Iteration 294 / 300) loss: 0.044976\n",
      "(Iteration 295 / 300) loss: 0.064509\n",
      "(Iteration 296 / 300) loss: 0.061888\n",
      "(Iteration 297 / 300) loss: 0.066962\n",
      "(Iteration 298 / 300) loss: 0.028010\n",
      "(Iteration 299 / 300) loss: 0.018962\n",
      "(Iteration 300 / 300) loss: 0.056633\n",
      "(Epoch 15 / 15) train acc: 0.992000; val_acc: 0.374000\n"
     ]
    }
   ],
   "source": [
    "# Object of Convolutional Neural Network\n",
    "model = ConvNet(weight_scale=1e-2)\n",
    "num_train = 1000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solver = Solver(model, small_data,\n",
    "                num_epochs=15, batch_size=50,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                  'learning_rate': 1e-3,\n",
    "                },\n",
    "                verbose=True, print_every=1)\n",
    "\n",
    "solver.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 32, 32)\n",
      "1 (1, 32, 16, 16)\n",
      "(1, 100)\n",
      "[[ 1.3289769  -1.6131816  -1.2976675   0.19625357  2.8078105   1.05697\n",
      "  -4.2801557  11.192909   -8.704842    3.5583096 ]]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image \n",
    "import numpy as np\n",
    "img = Image.open(\"D:/horse.jpg\",\"r\")\n",
    "img.load()\n",
    "img = img.resize((32, 32))\n",
    "data = np.asarray( img, dtype=\"int32\" )\n",
    "\n",
    "data = np.expand_dims(data,axis=0)\n",
    "data=data.transpose(0,3,1,2)\n",
    "print (data.shape)\n",
    "scores=model.loss(data)\n",
    "\n",
    "print (scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x294aa8f4f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
