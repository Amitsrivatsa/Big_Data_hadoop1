{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERROR FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing Numpy \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TESTING CONV_FORWARD_NAIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a convolutional layer.\n",
    "\n",
    "    The input consists of N data points, each with C channels, height H and width\n",
    "    W. We convolve each input with F different filters, where each filter spans\n",
    "    all C channels and has height HH and width HH.\n",
    "\n",
    "    Input:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - w: Filter weights of shape (F, C, HH, WW)\n",
    "    - b: Biases, of shape (F,)\n",
    "    - conv_param: A dictionary with the following keys:\n",
    "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "        horizontal and vertical directions.\n",
    "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "      H' = 1 + (H + 2 * pad - HH) / stride\n",
    "      W' = 1 + (W + 2 * pad - WW) / stride\n",
    "    - cache: (x, w, b, conv_param)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "   \n",
    "    N, C, H, W = x.shape\n",
    "    F, C, HH, WW = w.shape\n",
    "    S = conv_param['stride']\n",
    "    P = int(conv_param[\"pad\"])\n",
    "  \n",
    "    # Add padding to each image\n",
    "    x_pad = np.pad(x, [(0, 0), (0, 0), (P, P), (P, P)], mode=\"constant\")\n",
    "    \n",
    "    # Size of the output\n",
    "    Hh =1 + (H + 2 * P - HH) / S\n",
    "    Hw =1 + (W + 2 * P - WW) / S\n",
    "    \n",
    "    Hh=int(Hh)\n",
    "    Hw=int(Hw)\n",
    "    out1=np.zeros((N,F,Hh,Hw))\n",
    "    \n",
    "   \n",
    "    for n in range(N):  # First, iterate over all the images\n",
    "        for f in range(F):  # Second, iterate over all the kernels\n",
    "            for k in range(Hh):\n",
    "                for l in range(Hw):\n",
    "                    out1[n, f, k, l] = np.sum(\n",
    "                        x_pad[n, :, k * S:k * S + HH, l * S:l * S + WW] * w[f, :]) + b[f]\n",
    "\n",
    "    cache = (x, w, b, conv_param)\n",
    "    return out1, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_naive\n",
      "difference:  2.21214764175e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
    "correct_out = np.array([[[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           \n",
    "                           \n",
    "                           \n",
    "                           \n",
    "                           \n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]]])\n",
    "\n",
    "# Compare your output to ours; difference should be around 1e-8\n",
    "print ('Testing conv_forward_naive')\n",
    "print ('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING CONV_BACKWARD_NAIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "  \"\"\"\n",
    "  Evaluate a numeric gradient for a function that accepts a numpy\n",
    "  array and returns a numpy array.\n",
    "  \"\"\"\n",
    "  grad = np.zeros_like(x)\n",
    "  it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "  while not it.finished:\n",
    "    ix = it.multi_index\n",
    "    \n",
    "    oldval = x[ix]\n",
    "    x[ix] = oldval + h\n",
    "    pos = f(x).copy()\n",
    "    x[ix] = oldval - h\n",
    "    neg = f(x).copy()\n",
    "    x[ix] = oldval\n",
    "    \n",
    "    grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "    it.iternext()\n",
    "  return grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a convolutional layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives.\n",
    "    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x\n",
    "    - dw: Gradient with respect to w\n",
    "    - db: Gradient with respect to b\n",
    "    \"\"\"\n",
    "    dx, dw, db = None, None, None\n",
    "    ##########################################################################\n",
    "    # TODO: Implement the convolutional backward pass.                          #\n",
    "    ##########################################################################\n",
    "    x, w, b, conv_param = cache\n",
    "    P = int(conv_param['pad'])\n",
    "    x_pad = np.pad(x, ((0,), (0,), (P,), (P,)), 'constant')\n",
    "\n",
    "    N, C, H, W = x.shape\n",
    "    F, C, HH, WW = w.shape\n",
    "    N, F, Hh, Hw = dout.shape\n",
    "    S = conv_param['stride']\n",
    "    \n",
    "    H=int(H)\n",
    "    W=int(W)\n",
    "    Hh=int(Hh)\n",
    "    Hw=int(Hw)\n",
    "    HH=int(HH)\n",
    "    WW=int(WW)\n",
    "\n",
    "    # For dw: Size (C,HH,WW)\n",
    "    # Brut force love the loops !\n",
    "    dw = np.zeros((F, C, HH, WW))\n",
    "    for fprime in range(F):\n",
    "        for cprime in range(C):\n",
    "            for i in range(HH):\n",
    "                for j in range(WW):\n",
    "                    sub_xpad = x_pad[:, cprime, i:i + Hh * S:S, j:j + Hw * S:S]\n",
    "                    dw[fprime, cprime, i, j] = np.sum(\n",
    "                        dout[:, fprime, :, :] * sub_xpad)\n",
    "\n",
    "    # For db : Size (F,)\n",
    "    db = np.zeros((F))\n",
    "    for fprime in range(F):\n",
    "        db[fprime] = np.sum(dout[:, fprime, :, :])\n",
    "\n",
    "    dx = np.zeros((N, C, H, W))\n",
    "    for nprime in range(N):\n",
    "        for i in range(H):\n",
    "            for j in range(W):\n",
    "                for f in range(F):\n",
    "                    for k in range(Hh):\n",
    "                        for l in range(Hw):\n",
    "                            mask1 = np.zeros_like(w[f, :, :, :])\n",
    "                            mask2 = np.zeros_like(w[f, :, :, :])\n",
    "                            if (i + P - k * S) < HH and (i + P - k * S) >= 0:\n",
    "                                mask1[:, i + P - k * S, :] = 1.0\n",
    "                            if (j + P - l * S) < WW and (j + P - l * S) >= 0:\n",
    "                                mask2[:, :, j + P - l * S] = 1.0\n",
    "                            w_masked = np.sum(\n",
    "                                w[f, :, :, :] * mask1 * mask2, axis=(1, 2))\n",
    "                            dx[nprime, :, i, j] += dout[nprime, f, k, l] * w_masked\n",
    "\n",
    "    return dx, dw, db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_backward_naive function\n",
      "dx error:  1.84513360417e-09\n",
      "dw error:  1.07054245962e-09\n",
      "db error:  9.27382403641e-12\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
    "dx, dw, db = conv_backward_naive(dout, cache)\n",
    "\n",
    "# Your errors should be around 1e-9'\n",
    "print ('Testing conv_backward_naive function')\n",
    "print ('dx error: ', rel_error(dx, dx_num))\n",
    "print ('dw error: ', rel_error(dw, dw_num))\n",
    "print ('db error: ', rel_error(db, db_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING MAX_POOL_NAIVE_FORWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_forward_naive(x, pool_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a max pooling layer.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C, H, W)\n",
    "    - pool_param: dictionary with the following keys:\n",
    "      - 'pool_height': The height of each pooling region\n",
    "      - 'pool_width': The width of each pooling region\n",
    "      - 'stride': The distance between adjacent pooling regions\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data (N,C,H1,W1)\n",
    "    - cache: (x, pool_param)\n",
    "\n",
    "    where H1 = (H-Hp)/S+1\n",
    "    and W1 = (W-Wp)/S+1\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Hp = pool_param['pool_height']\n",
    "    Wp = pool_param['pool_width']\n",
    "    S = pool_param['stride']\n",
    "    N, C, H, W = x.shape\n",
    "    H1 = (H - Hp) / S + 1\n",
    "    W1 = (W - Wp) / S + 1\n",
    "    H1=int(H1)\n",
    "    W1=int(W1)\n",
    "    out = np.zeros((N, C, H1, W1))\n",
    "    for n in range(N):\n",
    "        for c in range(C):\n",
    "            for k in range(H1):\n",
    "                for l in range(W1):\n",
    "                    out[n, c, k, l] = np.max(\n",
    "                        x[n, c, k * S:k * S + Hp, l * S:l * S + Wp])\n",
    "\n",
    "    cache = (x, pool_param)\n",
    "    return out, cache\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_forward_naive function:\n",
      "difference:  4.16666651573e-08\n"
     ]
    }
   ],
   "source": [
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
    "\n",
    "out, _ = max_pool_forward_naive(x, pool_param)\n",
    "\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "# Compare your output with ours. Difference should be around 1e-8.\n",
    "print ('Testing max_pool_forward_naive function:')\n",
    "print ('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING MAX_POOL_NAIVE_BACKWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a max pooling layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives\n",
    "    - cache: A tuple of (x, pool_param) as in the forward pass.\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx = None\n",
    "    ##########################################################################\n",
    "    # TODO: Implement the max pooling backward pass                             #\n",
    "    ##########################################################################\n",
    "    x, pool_param = cache\n",
    "    Hp = pool_param['pool_height']\n",
    "    Wp = pool_param['pool_width']\n",
    "    S = pool_param['stride']\n",
    "    N, C, H, W = x.shape\n",
    "    H1 = (H - Hp) / S + 1\n",
    "    W1 = (W - Wp) / S + 1\n",
    "    \n",
    "    H1=int(H1)\n",
    "    W1=int(W1)\n",
    "    Hp=int(Hp)\n",
    "    Wp=int(Wp)\n",
    "    H=int(H)\n",
    "    W=int(W)\n",
    "\n",
    "    dx = np.zeros((N, C, H, W))\n",
    "    for nprime in range(N):\n",
    "        for cprime in range(C):\n",
    "            for k in range(H1):\n",
    "                for l in range(W1):\n",
    "                    x_pooling = x[nprime, cprime, k *\n",
    "                                  S:k * S + Hp, l * S:l * S + Wp]\n",
    "                    maxi = np.max(x_pooling)\n",
    "                    x_mask = x_pooling == maxi\n",
    "                    dx[nprime, cprime, k * S:k * S + Hp, l * S:l *\n",
    "                        S + Wp] += dout[nprime, cprime, k, l] * x_mask\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_backward_naive function:\n",
      "dx error:  3.27562829233e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3, 2, 8, 8)\n",
    "dout = np.random.randn(3, 2, 4, 4)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_naive(x, pool_param)[0], x, dout)\n",
    "\n",
    "out, cache = max_pool_forward_naive(x, pool_param)\n",
    "dx = max_pool_backward_naive(dout, cache)\n",
    "\n",
    "# Your error should be around 1e-12\n",
    "print ('Testing max_pool_backward_naive function:')\n",
    "print ('dx error: ', rel_error(dx, dx_num))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
